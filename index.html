<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Text Mining and Word2vec</title>
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/manifest.json">
		<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
		<meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
		<script type="text/x-mathjax-config">
		 MathJax.Hub.Config({
		   tex2jax: {
		     inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		     processEscapes: true
		   }
		 });
		</script>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
		  <div class="slides">
		    <section>
		      <h2><b>LMLow 3: Text mining and word2vec</b></h2>
		      <img src="resources/w2v.png" alt="">
		    </section>
		    <section>
		    	A <b>word</b> as the basic unit of discrete data, defined as an item from a vocabulary indexed by $1, . . . , V$.
		    	<aside class="notes">
					Compared to image data which can analysed as a pixel grid or a matrix of RBG, or continuous data, how can we evaluate text? Well, the outcome of this 20 minute talk is hopefully to demystify how one might use text data in machine learning models or otherwise text mining applications they might have in their work.
				</aside>
		    </section>
		    <section>
		    	<h3><b>Count-based methods</b></h3>
		    	<p style="text-align: justify">Previously represented by simplified assumptions of sparse vectors. These methods represent text as a <b>bag of words</b> where a document is described by counting the number of co-occurrences of words across documents.</p>
		    	<aside class="notes">
		    		Spare vectors are where you use one-hot encoding to create a vector for each word in the vocabularly. In the picture in the slide, 5 vectors would be computed, with the dimensions of the vector equal to the size of the vocabulary. 1 represents the index that stands for the word, and 0 otherwise. 
		    	</aside>	
		    	<img src="resources/word2vec-one-hot.png">
		    	
		    	<aside class="notes">
		    		Documents are units of texts such as web pages, newspaper articles et cetera. In NLP speak, a collection of documents forms a corpus.
		    	</aside>	
		    </section>
		    <section>
		    	<h4><b>Latent semantic analysis</b></h4>
		    	<p style="text-align: justify; font-size: 70%">Term document matrix is used where rows are unique words and columns each document. TF-IDF transformation used to weight the importance of each word. SVD factorization used to group documents using similar words. Detects <b>latent semantic components</b> between documents (Deerwester et al., 1990).</p>
		    	<aside class="notes">Corresponds to the frequency of a term within a document divided by its frequency in the entire corpus. This downweights common, conventional stop words such as "the", "a", "it" which contribute less discriminate power to a document than terms which are document (or topic) specific.</aside>
		    	<video autoplay loop controls style="border: 3px solid black;" height="400px">
		    		<source data-src="resources/Topic_model_scheme.webm" type="video/webm"></source>
		    	</video>
		    	<p style="text-align: justify; font-size: 70%"></p>
		    	<aside class="notes">Factorize matrix with singular value decomposition which identifies the dimensions across which most of the variance (or co-occurrence) in the space of tf-idf is captured. Means that vectors of words that co-occur in certain documents, or exhibit similar variane across documents, end up closer together in this new vector space called a topic space.</aside>
		    </section>
		    <section>
		    	<h4><b>Shortcomings</b></h4>
		    	<p style="text-align: justify;">Co-occurrence encodes shallow topical similarity. Levy & Goldberg (2014) finds "hogwarts" close to Harry Potter terms, as opposed to terms recognised as functionally or conceptually similar - i.e. other elite educational institutions.</p> 
		    	<img src="resources/lsa_vs_embeds.png">
		    </section>
		 	<section>
		 		<h3><b>Prediction-based methods</b></h3>
		 		<p style="text-align: justify;">Purport Baroni et al.'s (2014) distributional hypothesis: "words that appear in the same contexts share semantic meaning."</p> 

		 		<p style="text-align: justify;">Instead of co-occurrence counts, predictive methods take raw, tokenised text as input and learn word vectors by predicting its surrounding context.</p>
		 	</section>
		    <section>
		       <p>Word2vec learns a distributed representation of each word in a vocabulary. A word embedding $W : \textit{words} \rightarrow \mathbb{R}^n$</p>
  			   <p>$W("king") = (0.24, -0.30, 0.67, . . . )$</p>
  			   <p>$W("queen") = (0.23, -0.35, 0.70, . . . )$</p>
  			    <p style="text-align: justify;">Dimensions of vector might be labelled by <b>royalty, masculinity, age </b> with each contributing to the abstract 'meaning' of a word in the vector space. Representation of each word is spread across every element of the vector.</p>
		    </section>
		    <section>
		    	<h3>How?</h3>
		    </section>
		    <section>
		    	<p>Word2vec uses a shallow neural network with a <b>single hidden layer</b>. Let's look at a simplified, trivial example that trains word2vec on the following sentences:</p>

		    	<p><i>"the dog saw a cat"</i></p>
		    	<p><i>"the dog chased the cat"</i></p>
		    	<p><i>"the cat climbed a tree"</i></p>

		    	<p>Our corpus contains eight unique words. Suppose we want to learn 3 dimensional word vectors and predict "climbed" from "cat"...</p>
		    </section>
		    <section>
		    	<h3>Neural Net Architecture</h3>
		    	<ol style="text-align: left; font-size: 70%;">
					<li>The input and output layer have as many neurons as there are words in the vocabulary (8).</li>
		    		<li>The hidden layer has the dimensionality of the resulting word vectors (3).</li>
		    		<li>The weights matrices $WI$ and $WO$ are $8 \times 3$ and $3 \times 8$, respectively.</li>
		    		<li>Matrices are randomly initialised to small random values.</li>		    	
		    	</ol>
		    	<img width="500px" height="300px" src="resources/nn_w2v.png">
		    </section>
		    <section>
		
		    	<p style="text-align:left;">In our case, $WI$ and $WO$ look like...</p>
		    	<p style="font-size: 70%;">$WI = \begin{bmatrix}
					-0.09 & 0.03 & 0.72 \\ 
					0.34 & 0.98 & 0.34 \\ 
					0.66 & -0.11 & -0.93\\ 
					-0.33 & -0.35 & -0.65 \\ 
					0.34 & 0.89 & 0.11 \\ 
					0.88 & 0.76 & 0.23\\ 
					-0.84 & -0.29 & 0.87 \\ 
					0.11 & -0.76 & 0.33
					\end{bmatrix}$</p>
				<p style="font-size: 70%;">$WO = 
					\begin{bmatrix}
					0.33 & 0.76  & 0.85 & -0.99 & -0.44  &  0.39 & 0.38 &  0.11 \\ 
					0.17 & 0.93 & 0.65 & -0.45 & 0.12 & -0.09 & 0.56 & 0.13\\ 
					-0.08 & 0.93 & 0.74 & 0.85  & -0.34 & -0.78 & 0.33 &  0.71
					\end{bmatrix}$</p>

		    </section>
			<section>
				<h3><b>Reminder...</b></h3>
				<p style="text-align:justify;">Our objective is to learn word relationships (between "cat" and "climbed", for example). Training proceeds by taking a one-hot encoded vector for each word. </p>
		    	<ul style="text-align: left;">
		    		<li>Input vector for cat is $[0,1,0,0,0,0,0,0]^t$</li>
		    		<li>Target vector for climbed is $[0,0,0,1,0,0,0,0]^t$</li>
		    	</ul>
				<aside class="notes">In word embedding terminology, the word cat is referred to as the context word, and the world climbed is referred as the target word.</aside>
			</section>
			<section>
				<h3>Learning (1)</h3>
				<p style="text-align:justify;">Since our input vectors are one-hot, the matrix multiplication by weight matrix $WI$ amounts to simply copying the input word vector to the hidden layer ($H^t$).</p>
				<p>$[0,1,0,0,0,0,0,0]^t \times WI = [0.34 , 0.98 , 0.34]$ </p>
				<p>$H^t = [0.34 , 0.98 , 0.34]$</p>
			</section>
			<section>
				<h3>Learning (2)</h3>
				<p style="text-align:justify;">Carrying out a similar matrix multiplication between the hidden layer ($H^t$) and $WO$ we find...</p>
				<p>$H^t \times WO = [0.25, 1.49, 1.18, -0.49, -0.15, -0.22, 0.79, 0.41]$</p>
				<p>This creates what is known as the <b>activation vector</b>.</p>

			</section>
			<section>
				<h3>Learning (3)</h3>
				<p style="text-align:justify;">Since our objective is to produce probabilities for words, we need the sum of neuron outputs to add to one. So, we convert the activation values of the output layer neurons to probabilities using the <b>softmax function</b>.</p>
				<p>$$P(word_k|word_{context}) = \frac{exp(activation(k))}{\sum\limits_{n=1}^V exp(activation(n))}$$</p>
			</section>
			<section>
				<h3 style="margin-top:-50px;">Learning (4)</h3>
				<p style="text-align:justify;">So, applying the softmax function, the probabilities for eight words in our "corpus" are:</p>
				<p style="text-align:justify;">$[0.09, 0.30, 0.22, \textbf{0.04}, 0.06, 0.05,0.15, 0.10]$.</p>
				<p style="text-align:justify;">The probability in bold represents our target word, "climbed". Given our target vector $[0,0,0,1,0,0,0,0]^t$. The error vector is computed by subtracting the probability vector from the target vector. Error is used to update the matrices for $WO$ and $WI$ using backpropagation.</p>
			</section>

			<section>
				<h3 style="margin-top:-50px;">Summary</h3>
				<p style="text-align: justify;">After training, word vectors are copied from $WI$, which have been updated by backpropagation. Vectors of words appearing in similar contexts will be closer (cosine distance). Training proceeds iteratively by presenting different word pairs from the corpus.</p> 
				<img width="25%" height="25%" src="resources/simplistic-term-vector-mode.png">
				<!-- <p style="text-align: justify;">Generally, word2vec comes in two flavours: CBOW and skip-gram.</p> -->
				<aside class="notes">CBOW context is represented by multiple words for a given target word. Skip-gram feeds a target word to the input layer, but the output layer of the neural network is replicated multiple times to accomodate the chosen number of context words. </aside>

			</section>
	
			<section>
			  <aside class="notes">Two flavours: tensorflow and gensim. Tensorflow generally requires a lot more boilerplate code but </aside>
			  <p>Gensim: "<i>topic modelling for humans</i>"</p>
			  <pre><code data-trim>
				from gensim.models import Word2Vec

				sentences = [["the", "dog", "saw", "a", "cat"], 
				         ["the", "dog", "chased" ,"the" ,"cat"],  
				         ["the", "cat", "climbed", "a", "tree"]]

				model = Word2Vec(sentences, min_count=0, window=1, size=3) 
				            
				model.wv.similarity('cat', 'chased')

				## Out[45]: 0.90155835391663142

				model['cat']

				## Out[48]: array([ 0.06525704,  0.02372613,  0.04815513], dtype=float32)
			  </code></pre>
			</section>
			<section>
			  <p style="margin-top:-50px;">Visualise learnt embeddings using PCA</p>
			  <pre style="font-size:30%;"><code data-trim>
				from sklearn.decomposition import PCA
				from matplotlib import pyplot

				X = model[model.wv.vocab]

				result = PCA(n_components=2).fit_transform(X)

				# create a scatter plot of the projection
				pyplot.scatter(result[:, 0], result[:, 1])
				words = list(model.wv.vocab)
				for i, word in enumerate(words):
				    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
				pyplot.show()
			  </code></pre>
			  <img width="500px" height="300px" src="resources/pca.png">
			</section>

		    <section>
		    	<P>But what can we do with word embeddings?</P>
		    </section>

		    <section>
		    	<h3>Application (1)</h3>
		    	<p style="text-align:justify;">Spotify abstract the ideas behind word2vec by treating songs as words, and other songs in a playlist as their surrounding "context window".</p>
		    	<img src="resources/spotify-logo2.jpg">

		    	<p style="text-align:justify;">In order to recommend songs to a user, Spotify examines a neighbourhood of 'song embeddings' of songs the user already likes!</p>
		    </section>

		    <section> 
		    	<h3>Application (2)</h3>
		    	<p style="font-size:70%;">Machine translation. Words from two different languages can be embedded in a single, shared space (Socher et al. 2013).</p>
		    	<img width="500px" height="300px" src="resources/t-sne.png" alt="">
		    	<p style="font-size:70%;">In light of our discussion, we observed word vectors pull similar words closer together, so using a few translation as anchor points, a mapping between English and Mandarin words can be created.</p>
		    </section>

		    <section>
		    	<p>Thanks for learning machine learning!</p>
		    </section>

		  </div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				math: {
					// mathjax: 'http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js' // online
					mathjax: 'node_modules/mathjax/MathJax.js' //offline-local
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
    	            { src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
